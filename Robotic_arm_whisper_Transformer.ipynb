{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime transformers soundfile\n",
        "!pip install git+https://github.com/facebookresearch/fairseq.git\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xalLnk6PqI3W",
        "outputId": "255d90ce-abc1-47a7-cf3f-e231376d848e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-m91lxdgi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-m91lxdgi\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=ce168c488d4b3be28bf37dc545da0986c4a21c562f98f34160e98da8fa59238a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7y664sbb/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "import struct\n",
        "from whisper import Whisper, ModelDimensions\n",
        "\n",
        "# Specify file paths directly\n",
        "fname_inp = '/content/drive/MyDrive/Models/ggml-base.en.bin'\n",
        "dir_out = '.'  # Current directory\n",
        "fname_out = 'torch-model.pt'\n",
        "\n",
        "# Open the ggml file\n",
        "with open(fname_inp, \"rb\") as f:\n",
        "    # Read magic number and hyperparameters\n",
        "    magic_number, n_vocab, n_audio_ctx, n_audio_state, n_audio_head, n_audio_layer, n_text_ctx, n_text_state, n_text_head, n_text_layer, n_mels, use_f16 = struct.unpack(\"12i\", f.read(48))\n",
        "    print(f\"Magic number: {magic_number}\")\n",
        "    print(f\"Vocab size: {n_vocab}\")\n",
        "    print(f\"Audio context size: {n_audio_ctx}\")\n",
        "    print(f\"Audio state size: {n_audio_state}\")\n",
        "    print(f\"Audio head size: {n_audio_head}\")\n",
        "    print(f\"Audio layer size: {n_audio_layer}\")\n",
        "    print(f\"Text context size: {n_text_ctx}\")\n",
        "    print(f\"Text head size: {n_text_head}\")\n",
        "    print(f\"Mel size: {n_mels}\")\n",
        "\n",
        "    # Read mel filters\n",
        "    filters_shape_0 = struct.unpack(\"i\", f.read(4))[0]\n",
        "    print(f\"Filters shape 0: {filters_shape_0}\")\n",
        "    filters_shape_1 = struct.unpack(\"i\", f.read(4))[0]\n",
        "    print(f\"Filters shape 1: {filters_shape_1}\")\n",
        "\n",
        "    mel_filters = np.zeros((filters_shape_0, filters_shape_1))\n",
        "    for i in range(filters_shape_0):\n",
        "        for j in range(filters_shape_1):\n",
        "            mel_filters[i][j] = struct.unpack(\"f\", f.read(4))[0]\n",
        "\n",
        "    # Read tokenizer tokens\n",
        "    bytes_data = f.read(4)\n",
        "    num_tokens = struct.unpack(\"i\", bytes_data)[0]\n",
        "    tokens = {}\n",
        "    for _ in range(num_tokens):\n",
        "        token_len = struct.unpack(\"i\", f.read(4))[0]\n",
        "        token = f.read(token_len)\n",
        "        tokens[token] = {}\n",
        "\n",
        "    # Read model variables\n",
        "    model_state_dict = OrderedDict()\n",
        "    while True:\n",
        "        try:\n",
        "            n_dims, name_length, ftype = struct.unpack(\"iii\", f.read(12))\n",
        "        except struct.error:\n",
        "            break  # End of file\n",
        "\n",
        "        dims = [struct.unpack(\"i\", f.read(4))[0] for _ in range(n_dims)]\n",
        "        dims = dims[::-1]\n",
        "        name = f.read(name_length).decode(\"utf-8\")\n",
        "\n",
        "        if ftype == 1:  # f16\n",
        "            data = np.fromfile(f, dtype=np.float16, count=np.prod(dims)).reshape(dims)\n",
        "        else:  # f32\n",
        "            data = np.fromfile(f, dtype=np.float32, count=np.prod(dims)).reshape(dims)\n",
        "\n",
        "        if name in [\"encoder.conv1.bias\", \"encoder.conv2.bias\"]:\n",
        "            data = data[:, 0]\n",
        "\n",
        "        model_state_dict[name] = torch.from_numpy(data)\n",
        "\n",
        "# Create Whisper model with correct dimensions\n",
        "dims = ModelDimensions(\n",
        "    n_mels=n_mels,\n",
        "    n_audio_ctx=n_audio_ctx,\n",
        "    n_audio_state=n_audio_state,\n",
        "    n_audio_head=n_audio_head,\n",
        "    n_audio_layer=n_audio_layer,\n",
        "    n_text_ctx=n_text_ctx,\n",
        "    n_text_state=n_text_state,\n",
        "    n_text_head=n_text_head,\n",
        "    n_text_layer=n_text_layer,\n",
        "    n_vocab=n_vocab,\n",
        ")\n",
        "model = Whisper(dims)\n",
        "\n",
        "# Load the state dict into the model\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Save the model in PyTorch format\n",
        "torch.save(model.state_dict(), fname_out)\n",
        "\n",
        "print(f\"Model successfully converted and saved to {fname_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_CLT0H3PgYB",
        "outputId": "a57ca249-002c-4ba5-d69d-9be2b9c6dde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magic number: 1734831468\n",
            "Vocab size: 51864\n",
            "Audio context size: 1500\n",
            "Audio state size: 512\n",
            "Audio head size: 8\n",
            "Audio layer size: 6\n",
            "Text context size: 448\n",
            "Text head size: 8\n",
            "Mel size: 80\n",
            "Filters shape 0: 80\n",
            "Filters shape 1: 201\n",
            "Model successfully converted and saved to torch-model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "import struct\n",
        "from whisper import Whisper, ModelDimensions\n",
        "\n",
        "# Specify file paths directly\n",
        "fname_inp = '/content/drive/MyDrive/Models/ggml-base.en.bin'\n",
        "dir_out = '.'  # Current directory\n",
        "fname_out = 'torch-model1.pt'\n",
        "\n",
        "# Open the ggml file\n",
        "with open(fname_inp, \"rb\") as f:\n",
        "    # Read magic number and hyperparameters\n",
        "    magic_number, n_vocab, n_audio_ctx, n_audio_state, n_audio_head, n_audio_layer, n_text_ctx, n_text_state, n_text_head, n_text_layer, n_mels, use_f16 = struct.unpack(\"12i\", f.read(48))\n",
        "    print(f\"Magic number: {magic_number}\")\n",
        "    print(f\"Vocab size: {n_vocab}\")\n",
        "    print(f\"Audio context size: {n_audio_ctx}\")\n",
        "    print(f\"Audio state size: {n_audio_state}\")\n",
        "    print(f\"Audio head size: {n_audio_head}\")\n",
        "    print(f\"Audio layer size: {n_audio_layer}\")\n",
        "    print(f\"Text context size: {n_text_ctx}\")\n",
        "    print(f\"Text head size: {n_text_head}\")\n",
        "    print(f\"Mel size: {n_mels}\")\n",
        "\n",
        "    # Read mel filters\n",
        "    filters_shape_0 = struct.unpack(\"i\", f.read(4))[0]\n",
        "    print(f\"Filters shape 0: {filters_shape_0}\")\n",
        "    filters_shape_1 = struct.unpack(\"i\", f.read(4))[0]\n",
        "    print(f\"Filters shape 1: {filters_shape_1}\")\n",
        "\n",
        "    mel_filters = np.zeros((filters_shape_0, filters_shape_1))\n",
        "    for i in range(filters_shape_0):\n",
        "        for j in range(filters_shape_1):\n",
        "            mel_filters[i][j] = struct.unpack(\"f\", f.read(4))[0]\n",
        "\n",
        "    # Read tokenizer tokens (this part might not be necessary, but I'm leaving it as is)\n",
        "    bytes_data = f.read(4)\n",
        "    num_tokens = struct.unpack(\"i\", bytes_data)[0]\n",
        "    tokens = {}\n",
        "    for _ in range(num_tokens):\n",
        "        token_len = struct.unpack(\"i\", f.read(4))[0]\n",
        "        token = f.read(token_len)\n",
        "        tokens[token] = {}\n",
        "\n",
        "    # Read model variables\n",
        "    model_state_dict = OrderedDict()\n",
        "    while True:\n",
        "        try:\n",
        "            n_dims, name_length, ftype = struct.unpack(\"iii\", f.read(12))\n",
        "        except struct.error:\n",
        "            break  # End of file\n",
        "\n",
        "        dims = [struct.unpack(\"i\", f.read(4))[0] for _ in range(n_dims)]\n",
        "        dims = dims[::-1]\n",
        "        name = f.read(name_length).decode(\"utf-8\")\n",
        "\n",
        "        if ftype == 1:  # f16\n",
        "            data = np.fromfile(f, dtype=np.float16, count=np.prod(dims)).reshape(dims)\n",
        "        else:  # f32\n",
        "            data = np.fromfile(f, dtype=np.float32, count=np.prod(dims)).reshape(dims)\n",
        "\n",
        "        if name in [\"encoder.conv1.bias\", \"encoder.conv2.bias\"]:\n",
        "            data = data[:, 0]\n",
        "\n",
        "        model_state_dict[name] = torch.from_numpy(data)\n",
        "\n",
        "# Create Whisper model with correct dimensions\n",
        "dims = ModelDimensions(\n",
        "    n_mels=n_mels,\n",
        "    n_audio_ctx=n_audio_ctx,\n",
        "    n_audio_state=n_audio_state,\n",
        "    n_audio_head=n_audio_head,\n",
        "    n_audio_layer=n_audio_layer,\n",
        "    n_text_ctx=n_text_ctx,\n",
        "    n_text_state=n_text_state,\n",
        "    n_text_head=n_text_head,\n",
        "    n_text_layer=n_text_layer,\n",
        "    n_vocab=n_vocab,\n",
        ")\n",
        "model = Whisper(dims)\n",
        "\n",
        "# Load the state dict into the model\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "# --- IMPORTANT: Save the model AND dimensions ---\n",
        "torch.save({\n",
        "    'dims': dims,        # Save the ModelDimensions object\n",
        "    'model_state_dict': model.state_dict()\n",
        "}, fname_out)\n",
        "\n",
        "print(f\"Model successfully converted and saved to {fname_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPWfP5LzJWox",
        "outputId": "29378b34-c622-4ea4-a12f-829631b6f81e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magic number: 1734831468\n",
            "Vocab size: 51864\n",
            "Audio context size: 1500\n",
            "Audio state size: 512\n",
            "Audio head size: 8\n",
            "Audio layer size: 6\n",
            "Text context size: 448\n",
            "Text head size: 8\n",
            "Mel size: 80\n",
            "Filters shape 0: 80\n",
            "Filters shape 1: 201\n",
            "Model successfully converted and saved to torch-model1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "\n",
        "# Load the Whisper model\n",
        "\n",
        "# 1. Load the checkpoint dictionary\n",
        "checkpoint = torch.load(\"torch-model1.pt\")\n",
        "\n",
        "# 2. Get the ModelDimensions object directly\n",
        "dims = checkpoint['dims']  # <---- No need to unpack\n",
        "\n",
        "# 3. Create the Whisper model using the loaded dims\n",
        "model = whisper.Whisper(dims)\n",
        "\n",
        "# 4. Load only the model weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# Prepare dummy inputs\n",
        "audio_dummy = whisper.load_audio(\"/content/drive/MyDrive/Models/jfk.wav\")\n",
        "audio_dummy = whisper.pad_or_trim(audio_dummy)\n",
        "mel_dummy = whisper.log_mel_spectrogram(audio_dummy).to(model.device)\n",
        "mel_dummy = mel_dummy[None, :] # Add batch dimension\n",
        "\n",
        "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.sot]], dtype=torch.long)\n",
        "\n",
        "# Set the model to inference mode\n",
        "model.eval()\n",
        "\n",
        "# --- Export the encoder ---\n",
        "torch.onnx.export(\n",
        "    model.encoder,\n",
        "    (mel_dummy,),\n",
        "    \"encoder.onnx\",\n",
        "    input_names=[\"mel\"],\n",
        "    output_names=[\"encoder_output\"],\n",
        "    dynamic_axes={\n",
        "        \"mel\": {0: \"batch\", 1: \"time\"},\n",
        "        \"encoder_output\": {0: \"batch\", 1: \"encoder_sequence\"}\n",
        "    }\n",
        ")\n",
        "print(\"Encoder exported to encoder.onnx\")\n",
        "\n",
        "# --- Export the decoder ---\n",
        "torch.onnx.export(\n",
        "    model.decoder,\n",
        "    (decoder_input_ids, model.encoder(mel_dummy)),\n",
        "    \"decoder.onnx\",\n",
        "    input_names=[\"tokens\", \"encoder_output\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\n",
        "        \"tokens\": {0: \"batch\", 1: \"text_length\"},\n",
        "        \"encoder_output\": {0: \"batch\", 1: \"encoder_sequence\"},\n",
        "        \"logits\": {0: \"batch\", 1: \"text_length\"}\n",
        "    }\n",
        ")\n",
        "print(\"Decoder exported to decoder.onnx\")"
      ],
      "metadata": {
        "id": "wLy_NN6JlluQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa012d0-62e6-4716-80bd-e797a69a668a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/model.py:166: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder exported to encoder.onnx\n",
            "Decoder exported to decoder.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "import onnx\n",
        "\n",
        "# Load the Whisper model\n",
        "model_path = \"/content/torch-model1.pt\"  # Replace with your model path\n",
        "checkpoint = torch.load(model_path)\n",
        "dims = checkpoint['dims']\n",
        "model = whisper.Whisper(dims)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Prepare dummy inputs\n",
        "audio_dummy = whisper.load_audio(\"/content/drive/MyDrive/Models/jfk.wav\")\n",
        "audio_dummy = whisper.pad_or_trim(audio_dummy)\n",
        "mel_dummy = whisper.log_mel_spectrogram(audio_dummy).to(model.device)\n",
        "mel_dummy = mel_dummy[None, :]\n",
        "\n",
        "tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.sot]], dtype=torch.long)\n",
        "\n",
        "# Set the model to inference mode\n",
        "model.eval()\n",
        "\n",
        "# --- Export the combined model ---\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (mel_dummy, decoder_input_ids),\n",
        "    \"whisper_combined.onnx\",\n",
        "    input_names=[\"mel\", \"decoder_input_ids\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\n",
        "        \"mel\": {0: \"batch\", 1: \"time\"},\n",
        "        \"decoder_input_ids\": {0: \"batch\", 1: \"text_length\"},\n",
        "        \"logits\": {0: \"batch\", 1: \"text_length\"}\n",
        "    },\n",
        "    opset_version=14\n",
        ")\n",
        "print(\"Combined Whisper model exported to whisper_combined.onnx\")"
      ],
      "metadata": {
        "id": "rmJ5G-Pallro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30d0da2-5d97-4201-9147-6a52ea5640c4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/model.py:166: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Whisper model exported to whisper_combined.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "# --- Load Whisper Model in PyTorch ---\n",
        "model = whisper.load_model(\"base.en\")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base.en\")\n",
        "\n",
        "# --- Load and Preprocess Audio ---\n",
        "audio_file = \"/content/drive/MyDrive/Models/jfk.wav\"\n",
        "audio = whisper.load_audio(audio_file)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "mel = mel.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# --- Transcribe using Whisper's decode method ---\n",
        "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
        "results = whisper.decode(model, mel, options) # 'results' is a list\n",
        "\n",
        "# --- Print the transcription ---\n",
        "print(\"PyTorch Transcription:\", results[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDqoHtcPWeyt",
        "outputId": "df6502c7-e798-40e3-9313-645e46b0ea1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Transcription: And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import whisper\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "# --- Load Whisper Model Architecture ---\n",
        "model = whisper.load_model(\"base.en\")  # Create a new model instance\n",
        "\n",
        "# 1. Load the checkpoint dictionary\n",
        "checkpoint = torch.load(\"torch-model1.pt\")\n",
        "\n",
        "# 2. Get the ModelDimensions object directly\n",
        "dims = checkpoint['dims']  # <---- No need to unpack\n",
        "\n",
        "# 3. Create the Whisper model using the loaded dims\n",
        "model = whisper.Whisper(dims)\n",
        "\n",
        "# 4. Load only the model weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# --- Load Tokenizer ---\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base.en\")\n",
        "\n",
        "# --- Load and Preprocess Audio ---\n",
        "audio_file = \"/content/drive/MyDrive/Models/jfk.wav\"\n",
        "audio = whisper.load_audio(audio_file)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "mel = mel.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# --- Transcribe using Whisper's decode method ---\n",
        "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
        "results = whisper.decode(model, mel, options)\n",
        "\n",
        "# --- Print the transcription ---\n",
        "print(\"PyTorch Transcription:\", results[0].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5oZxH14KuqB",
        "outputId": "628cb19f-21ca-4e00-e1de-758acf0b9672"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Transcription: And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import whisper\n",
        "\n",
        "# --- 1. Load ONNX models ---\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "sess_encoder = onnxruntime.InferenceSession(\"encoder.onnx\", sess_options=sess_options)\n",
        "sess_decoder = onnxruntime.InferenceSession(\"decoder.onnx\", sess_options=sess_options)\n",
        "\n",
        "# --- 2. Load audio and prepare mel spectrogram ---\n",
        "audio_file = \"/content/drive/MyDrive/Models/jfk.wav\"\n",
        "audio = whisper.load_audio(audio_file)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio)\n",
        "mel = mel.unsqueeze(0).numpy()\n",
        "\n",
        "# --- 3. Load tokenizer ---\n",
        "tokenizer = whisper.tokenizer.get_tokenizer(multilingual=False, language=\"en\", task=\"transcribe\")\n",
        "\n",
        "# --- 4. Set inference parameters ---\n",
        "max_tokens = 512\n",
        "temperature = 0\n",
        "# ... add other parameters like beam_size, best_of, etc. as needed ...\n",
        "\n",
        "# --- 5. ONNX Inference ---\n",
        "start_time = time.time()\n",
        "\n",
        "# Encode the audio\n",
        "encoder_output, = sess_encoder.run([\"encoder_output\"], {\"mel\": mel})\n",
        "\n",
        "# Initialize decoder input with start of sequence (sot) token\n",
        "tokens = [tokenizer.sot]\n",
        "\n",
        "# Loop to generate tokens\n",
        "for _ in range(max_tokens):\n",
        "    # Prepare decoder input\n",
        "    decoder_input = np.array([tokens], dtype=np.int64)\n",
        "\n",
        "    # Run the decoder\n",
        "    logits, = sess_decoder.run([\"logits\"], {\"tokens\": decoder_input, \"encoder_output\": encoder_output})\n",
        "\n",
        "    # Sample the next token (greedy decoding for now)\n",
        "    next_token = logits[0, -1].argmax()\n",
        "\n",
        "    # Append the token to the sequence\n",
        "    tokens.append(next_token)\n",
        "\n",
        "    # Stop if end-of-sequence (eot) token is generated\n",
        "    if next_token == tokenizer.eot:\n",
        "        break\n",
        "\n",
        "# Decode the generated tokens\n",
        "transcription = tokenizer.decode(tokens)\n",
        "\n",
        "\n",
        "transcription = transcription.replace(\"<|startoftranscript|>\", \"\")\n",
        "transcription = transcription.replace(\"<|notimestamps|>\", \"\")\n",
        "transcription = transcription.replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"ONNX Inference Time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Transcription: {transcription}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5FqTgktOru8",
        "outputId": "b18e6343-f7b9-42c5-ce4c-644e38e1ccf1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Inference Time: 10.85 seconds\n",
            "Transcription:  And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import whisper\n",
        "\n",
        "# --- 1. Load the combined ONNX model ---\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "session = onnxruntime.InferenceSession(\"whisper_combined.onnx\", sess_options=sess_options)\n",
        "\n",
        "# --- 2. Load audio and prepare mel spectrogram ---\n",
        "audio_file = \"/content/drive/MyDrive/Models/jfk.wav\" # Replace with your audio file\n",
        "audio = whisper.load_audio(audio_file)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio)\n",
        "mel = mel.unsqueeze(0).numpy()  # Add batch dimension\n",
        "\n",
        "# --- 3. Load tokenizer ---\n",
        "tokenizer = whisper.tokenizer.get_tokenizer(multilingual=False, language=\"en\", task=\"transcribe\")\n",
        "\n",
        "# --- 4. Set inference parameters ---\n",
        "max_tokens = 512\n",
        "temperature = 0\n",
        "\n",
        "# --- 5. ONNX Inference ---\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize decoder input with start of sequence (sot) token\n",
        "tokens = [tokenizer.sot]\n",
        "\n",
        "# Loop to generate tokens\n",
        "for _ in range(max_tokens):\n",
        "    decoder_input = np.array([tokens], dtype=np.int64)\n",
        "\n",
        "    # Run the combined ONNX model\n",
        "    logits, = session.run([\"logits\"], {\"mel\": mel, \"decoder_input_ids\": decoder_input})\n",
        "\n",
        "    next_token = logits[0, -1].argmax()\n",
        "    tokens.append(next_token)\n",
        "\n",
        "    if next_token == tokenizer.eot:\n",
        "        break\n",
        "\n",
        "# Decode the generated tokens\n",
        "transcription = tokenizer.decode(tokens)\n",
        "\n",
        "# --- 6. Remove special tokens ---\n",
        "transcription = transcription.replace(\"<|startoftranscript|>\", \"\")\n",
        "transcription = transcription.replace(\"<|notimestamps|>\", \"\")\n",
        "transcription = transcription.replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"ONNX Inference Time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Transcription: {transcription}\")"
      ],
      "metadata": {
        "id": "L_3-xc89llpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36f83b7-2e57-4af8-f039-5cde5dd465bd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Inference Time: 70.96 seconds\n",
            "Transcription:  And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kb4NEJHHllmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}